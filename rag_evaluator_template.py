import os
import numpy as np

# Configura√ß√£o da API Key da OpenAI - usando m√∫ltiplas op√ß√µes
api_key_loaded = False
try:
    from decouple import config
    os.environ['OPENAI_API_KEY'] = config('OPENAI_API_KEY')
    api_key_loaded = True
    print("‚úì API Key carregada via python-decouple")
except ImportError:
    try:
        from dotenv import load_dotenv
        load_dotenv()
        if 'OPENAI_API_KEY' in os.environ:
            api_key_loaded = True
            print("‚úì API Key carregada via python-dotenv")
        else:
            print("AVISO: OPENAI_API_KEY n√£o encontrada no .env")
    except ImportError:
        print("AVISO: Nem python-decouple nem python-dotenv est√£o dispon√≠veis")
except Exception as e:
    print(f"AVISO: N√£o foi poss√≠vel carregar OPENAI_API_KEY do .env. Erro: {e}")

if not api_key_loaded and 'OPENAI_API_KEY' not in os.environ:
    print("\n" + "="*60)
    print("‚ö†Ô∏è  CONFIGURA√á√ÉO NECESS√ÅRIA")
    print("="*60)
    print("Para executar este script, voc√™ precisa configurar sua API Key da OpenAI.")
    print("Op√ß√µes:")
    print("1. Edite o arquivo .env e substitua 'your_openai_api_key_here' pela sua chave")
    print("2. Configure a vari√°vel de ambiente: set OPENAI_API_KEY=sua_chave_aqui")
    print("3. Execute: $env:OPENAI_API_KEY='sua_chave_aqui' (PowerShell)")
    print("="*60)
    
    # Dar uma chance ao usu√°rio de configurar manualmente
    user_key = input("Cole sua API Key da OpenAI aqui (ou pressione Enter para sair): ").strip()
    if user_key:
        os.environ['OPENAI_API_KEY'] = user_key
        api_key_loaded = True
        print("‚úì API Key configurada manualmente")
    else:
        print("Script cancelado. Configure a API Key e execute novamente.")
        exit(1)

# Importa√ß√µes do LangChain - com tratamento de erros
try:
    from langchain.chains.combine_documents import create_stuff_documents_chain
    from langchain.chains.retrieval import create_retrieval_chain
    from langchain.text_splitter import RecursiveCharacterTextSplitter
    from langchain_chroma import Chroma
    from langchain_community.document_loaders import PyPDFLoader
    from langchain_core.prompts import ChatPromptTemplate
    from langchain_openai import ChatOpenAI, OpenAIEmbeddings
    
    # Importa√ß√µes para RAG H√≠brido (se necess√°rio)
    from langchain_community.retrievers import BM25Retriever
    from langchain.retrievers import EnsembleRetriever
    
    print("‚úì Todas as importa√ß√µes do LangChain foram carregadas com sucesso")
except ImportError as e:
    print(f"ERRO CR√çTICO: Falha ao importar bibliotecas do LangChain: {e}")
    print("Certifique-se de que todas as depend√™ncias est√£o instaladas corretamente.")
    print("Execute: pip install -r requirements.txt")
    exit(1)

# --- Configura√ß√µes Globais Padr√£o (podem ser sobrescritas) ---
DEFAULT_DOCUMENTS_DIR = 'documents'
DEFAULT_PERSIST_DIR = 'db_eval' # Usar um diret√≥rio diferente para n√£o interferir com o app
DEFAULT_LLM_MODEL = "gpt-4o-mini" # Modelo mais econ√¥mico e r√°pido para testes

# --- 1. FUN√á√ïES AUXILIARES DE CARREGAMENTO E PREPARA√á√ÉO ---

def load_documents(documents_dir=DEFAULT_DOCUMENTS_DIR):
    """Carrega e splita todos os documentos PDF da pasta especificada."""
    print(f"Carregando documentos da pasta: {documents_dir}")
    docs_for_processing = []
    if not os.path.exists(documents_dir):
        print(f"ERRO: Pasta de documentos '{documents_dir}' n√£o encontrada.")
        return None, None # Retorna None para docs e splits

    pdf_files = [f for f in os.listdir(documents_dir) if f.endswith('.pdf')]
    if not pdf_files:
        print(f"Nenhum arquivo PDF encontrado em '{documents_dir}'.")
        return None, None

    for pdf_file in pdf_files:
        try:
            loader = PyPDFLoader(os.path.join(documents_dir, pdf_file))
            docs_for_processing.extend(loader.load())
        except Exception as e:
            print(f"Erro ao carregar o arquivo {pdf_file}: {e}")
            continue # Pula para o pr√≥ximo arquivo
    
    if not docs_for_processing:
        print("Nenhum documento PDF p√¥de ser carregado com sucesso.")
        return None, None

    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=400,
        length_function=len,
    )
    split_docs = text_splitter.split_documents(documents=docs_for_processing)
    print(f"Total de {len(split_docs)} chunks criados a partir dos documentos.")
    return docs_for_processing, split_docs # Retorna os documentos originais (para BM25) e os splits

def get_vector_store(split_documents, persist_directory=DEFAULT_PERSIST_DIR, force_recreate=False):
    """Carrega ou cria o Chroma vector store."""
    if not split_documents and not (os.path.exists(persist_directory) and not force_recreate):
        print("ERRO: Nenhum documento fornecido para criar o vector store e nenhum existente para carregar (ou recria√ß√£o for√ßada).")
        return None

    if os.path.exists(persist_directory) and not force_recreate:
        print(f"Carregando vector store existente de: {persist_directory}")
        try:
            vector_store = Chroma(
                persist_directory=persist_directory,
                embedding_function=OpenAIEmbeddings(),
            )
            return vector_store
        except Exception as e:
            print(f"Erro ao carregar vector store existente: {e}. Tentando recriar...")
            # Tratar como se fosse para recriar se o carregamento falhar
            if not split_documents:
                 print("ERRO: N√£o √© poss√≠vel recriar o vector store sem documentos divididos.")
                 return None
            force_recreate = True # For√ßa a recria√ß√£o

    if split_documents: # Entra aqui se force_recreate √© True ou se o diret√≥rio n√£o existe
        print(f"Criando novo vector store em: {persist_directory}")
        vector_store = Chroma.from_documents(
            documents=split_documents,
            embedding=OpenAIEmbeddings(),
            persist_directory=persist_directory,
        )
        return vector_store
    else:
        # Este caso n√£o deveria ser alcan√ßado se a l√≥gica anterior estiver correta
        print("ERRO: Incapaz de obter ou criar o vector store.")
        return None

def get_llm(model_name=DEFAULT_LLM_MODEL):
    """Retorna o modelo LLM configurado."""
    print(f"Inicializando LLM: {model_name}")
    return ChatOpenAI(model=model_name, temperature=0) # Temperatura 0 para respostas mais determin√≠sticas

def create_rag_chain_from_retriever(retriever, llm, system_prompt):
    """Cria uma cadeia RAG a partir de um retriever e um system prompt."""
    prompt = ChatPromptTemplate.from_messages([
        ("system", system_prompt),
        ("human", "{input}")
    ])
    question_answer_chain = create_stuff_documents_chain(llm=llm, prompt=prompt)
    chain = create_retrieval_chain(retriever=retriever, combine_docs_chain=question_answer_chain)
    return chain

# --- 2. DEFINI√á√ÉO DOS MODELOS/PIPELINES RAG PARA TESTE ---

# Exemplo de System Prompt (pode ser customizado por pipeline)
DEFAULT_SYSTEM_PROMPT = """Voc√™ √© um assistente de IA. Use o contexto fornecido para responder √† pergunta.
Se a informa√ß√£o n√£o estiver no contexto, diga que voc√™ n√£o sabe.
Contexto: {context}"""

def get_embedding_retriever(vector_store, k=3):
    """Retorna um retriever baseado em embeddings (Chroma)."""
    if not vector_store: return None
    return vector_store.as_retriever(search_kwargs={"k": k})

def get_hybrid_retriever(raw_documents, vector_store, bm25_k=2, embedding_k=2, ensemble_weights=[0.5, 0.5]):
    """Retorna um retriever h√≠brido (BM25 + Embeddings)."""
    if not raw_documents or not vector_store: return None
    print("Configurando retriever H√≠brido (BM25 + Embeddings)...")
    
    try:
        # BM25Retriever espera documentos com page_content
        # raw_documents j√° s√£o Document objects, ent√£o deve funcionar
        bm25_retriever = BM25Retriever.from_documents(raw_documents)
        bm25_retriever.k = bm25_k
        
        embedding_retriever = vector_store.as_retriever(search_kwargs={"k": embedding_k})
        
        ensemble_retriever = EnsembleRetriever(
            retrievers=[bm25_retriever, embedding_retriever],
            weights=ensemble_weights
        )
        return ensemble_retriever
    except Exception as e:
        print(f"Erro ao criar retriever h√≠brido: {e}")
        return None

# --- 3. DATASET DE AVALIA√á√ÉO ---
# Este dataset deve ser fornecido ou carregado de um arquivo.
# Formato esperado: lista de dicion√°rios, cada um com "id", "question", "ideal_answer".

# --- 4. FUN√á√ïES DE SCORING ---

def semantic_similarity_score(generated_answer, ideal_answer, embedding_model=None):
    """Calcula a similaridade de cosseno entre embeddings das respostas."""
    if not generated_answer or not ideal_answer:
        return 0.0
    
    _embedding_model = embedding_model if embedding_model else OpenAIEmbeddings()
    
    try:
        gen_emb = _embedding_model.embed_query(generated_answer)
        ideal_emb = _embedding_model.embed_query(ideal_answer)
        
        similarity = np.dot(gen_emb, ideal_emb) / (np.linalg.norm(gen_emb) * np.linalg.norm(ideal_emb))
        return max(0, float(similarity)) * 100 # Retorna como porcentagem, n√£o negativo
    except Exception as e:
        print(f"  Erro no semantic_similarity_score: {e}")
        return 0.0

# --- 5. ORQUESTRADOR DA AVALIA√á√ÉO ---

def run_evaluation(evaluation_dataset, rag_pipelines_to_test, llm, system_prompt=DEFAULT_SYSTEM_PROMPT):
    """
    Executa a avalia√ß√£o para um conjunto de dados e pipelines RAG.
    
    Args:
        evaluation_dataset (list): Lista de dicts com {"id", "question", "ideal_answer"}.
        rag_pipelines_to_test (dict): Dicion√°rio onde a chave √© o nome do pipeline 
                                      e o valor √© a inst√¢ncia do retriever.
                                      Ex: {"embedding_rag": retriever1, "hybrid_rag": retriever2}
        llm: A inst√¢ncia do modelo de linguagem.
        system_prompt (str): O prompt de sistema a ser usado.

    Returns:
        dict: Resultados da avalia√ß√£o, com scores para cada pipeline.
              Ex: {"embedding_rag": {"average_score": 75.0, "details": [...]}, ...}
    """
    if not evaluation_dataset:
        print("Dataset de avalia√ß√£o est√° vazio. Nada para testar.")
        return {}
    if not rag_pipelines_to_test:
        print("Nenhum pipeline RAG para testar.")
        return {}

    all_results = {}

    for pipeline_name, retriever in rag_pipelines_to_test.items():
        if retriever is None:
            print(f"Skipping pipeline '{pipeline_name}' pois o retriever n√£o foi inicializado.")
            all_results[pipeline_name] = {"average_score": 0.0, "details": [], "error": "Retriever n√£o inicializado"}
            continue

        print(f"\n--- Avaliando Pipeline: {pipeline_name} ---")
        rag_chain = create_rag_chain_from_retriever(retriever, llm, system_prompt)
        
        pipeline_scores = []
        pipeline_details = []

        for item in evaluation_dataset:
            question = item["question"]
            ideal_answer = item.get("ideal_answer") # .get() para ser flex√≠vel se n√£o houver resposta ideal

            print(f"  Avaliando Pergunta ID: {item.get('id', 'N/A')} - \"{question[:50]}...\"")
            
            try:
                response_payload = rag_chain.invoke({'input': question})
                generated_answer = response_payload.get('answer', "Resposta n√£o gerada.")
                
                score = 0.0
                if ideal_answer and generated_answer != "Resposta n√£o gerada.":
                    score = semantic_similarity_score(generated_answer, ideal_answer)
                
                pipeline_scores.append(score)
                pipeline_details.append({
                    "id": item.get("id", "N/A"),
                    "question": question,
                    "ideal_answer": ideal_answer,
                    "generated_answer": generated_answer,
                    "similarity_score": score,
                    "retrieved_context_count": len(response_payload.get('context', []))
                })
                print(f"    Score de Similaridade: {score:.2f}%")

            except Exception as e:
                print(f"    ERRO ao processar pergunta ID {item.get('id', 'N/A')}: {e}")
                pipeline_scores.append(0) # Penaliza em caso de erro
                pipeline_details.append({
                    "id": item.get("id", "N/A"),
                    "question": question,
                    "ideal_answer": ideal_answer,
                    "generated_answer": "ERRO NA GERA√á√ÉO",
                    "similarity_score": 0.0,
                    "error_message": str(e)
                })

        average_score = np.mean(pipeline_scores) if pipeline_scores else 0.0
        all_results[pipeline_name] = {
            "average_score": average_score,
            "details": pipeline_details
        }
        print(f"  Score M√©dio para {pipeline_name}: {average_score:.2f}%")
        
    return all_results

def print_summary_report(evaluation_results):
    """Imprime um relat√≥rio resumido dos scores."""
    print("\n\n--- Relat√≥rio Resumido dos Scores ---")
    if not evaluation_results:
        print("Nenhum resultado de avalia√ß√£o para reportar.")
        return

    for pipeline_name, results in evaluation_results.items():
        if "error" in results:
            print(f"Pipeline: {pipeline_name} - ERRO: {results['error']}")
        else:
            print(f"Pipeline: {pipeline_name}")
            print(f"  Score M√©dio de Similaridade: {results['average_score']:.2f}%")
    print("------------------------------------")

# --- 6. EXEMPLO DE USO ---

if __name__ == "__main__":
    print("üöÄ Iniciando script de avalia√ß√£o RAG...")
    print("="*60)

    # ETAPA 1: Carregar e Preparar Documentos
    print("üìÇ ETAPA 1: Carregando documentos...")
    raw_docs, split_docs = load_documents(documents_dir=DEFAULT_DOCUMENTS_DIR)
    
    if not split_docs:
        print("‚ùå N√£o foi poss√≠vel carregar ou processar documentos. Encerrando avalia√ß√£o.")
        exit()

    # Use force_recreate=True se quiser reconstruir o vector store do zero
    print("\nüóÑÔ∏è  ETAPA 2: Configurando vector store...")
    vector_store_instance = get_vector_store(split_docs, persist_directory=DEFAULT_PERSIST_DIR, force_recreate=False)

    if not vector_store_instance:
        print("‚ùå N√£o foi poss√≠vel obter o vector store. Encerrando avalia√ß√£o.")
        exit()

    # ETAPA 2: Configurar LLM
    print(f"\nü§ñ ETAPA 3: Configurando LLM ({DEFAULT_LLM_MODEL})...")
    llm_instance = get_llm(model_name=DEFAULT_LLM_MODEL)

    # ETAPA 3: Definir os Pipelines RAG para Testar
    print("\n‚öôÔ∏è  ETAPA 4: Configurando pipelines RAG...")
    pipelines = {}
    
    embedding_ret = get_embedding_retriever(vector_store_instance, k=3)
    if embedding_ret:
        pipelines["RAG_Base_Embeddings"] = embedding_ret
        print("‚úì Pipeline RAG Base (Embeddings) configurado")

    # Para o RAG H√≠brido, precisamos dos documentos brutos (antes do split para o vector store)
    if raw_docs: # Apenas tente criar o retriever h√≠brido se raw_docs foram carregados
        hybrid_ret = get_hybrid_retriever(raw_docs, vector_store_instance, bm25_k=2, embedding_k=2)
        if hybrid_ret:
            pipelines["RAG_Hibrido_BM25_Embeddings"] = hybrid_ret
            print("‚úì Pipeline RAG H√≠brido (BM25 + Embeddings) configurado")
    else:
        print("‚ö†Ô∏è  Documentos brutos n√£o carregados, retriever h√≠brido n√£o ser√° configurado.")

    # ETAPA 4: Definir o Dataset de Avalia√ß√£o
    print("\nüìù ETAPA 5: Preparando dataset de avalia√ß√£o...")
    CURRENT_EVALUATION_DATASET = [
        {
            "id": "CONTR_OBJ_001",
            "question": "Qual √© o objeto principal do Contrato de Participa√ß√£o em Grupo de Cons√≥rcio, previsto neste Regulamento?",
            "ideal_answer": "O objeto do Contrato √© regulamentar a participa√ß√£o do consorciado em um Grupo de Cons√≥rcio espec√≠fico, permitindo que o participante adquira uma cota com o 'cr√©dito de refer√™ncia' destinado √† aquisi√ß√£o de bens (aqui, ve√≠culos), conforme definido na 'Proposta de Ades√£o' e em conformidade com a Lei 11.795/2008 e a Resolu√ß√£o Bacen n¬∫ 285/2023."
        },
        {
            "id": "ADM_OBRIG_002",
            "question": "Quais s√£o as principais obriga√ß√µes da Administradora em rela√ß√£o ao Grupo de Cons√≥rcio?",
            "ideal_answer": "A Administradora deve, entre outras fun√ß√µes: Efetuar controle di√°rio das movimenta√ß√µes financeiras dos Grupos; Disponibilizar em cada Assembleia Geral Ordin√°ria (A.G.O.) o balancete patrimonial, demonstra√ß√£o dos recursos do cons√≥rcio e varia√ß√µes das disponibilidades do Grupo; Fornecer informa√ß√µes solicitadas pelos consorciados, desde que autorizadas; Lavrar atas das Assembleias; Encaminhar, junto ao boleto de cobran√ßa, a demonstra√ß√£o dos recursos e varia√ß√µes das disponibilidades do Grupo; Manter sistemas de controle operacional para exame pelo Bacen e pelos representantes dos consorciados."
        },
        {
            "id": "CONS_DEVER_003",
            "question": "Quais s√£o os deveres do Consorciado quanto √† atualiza√ß√£o cadastral e tratamento de dados?",
            "ideal_answer": "O consorciado deve manter seus dados cadastrais sempre atualizados (endere√ßo, e-mail, telefone, contas banc√°rias ou chave Pix), mesmo que esteja exclu√≠do; autorizar a inscri√ß√£o dos dados no banco de dados de 'bureau positivo' para decis√µes de cr√©dito; e zelar pelo sigilo e veracidade das informa√ß√µes, podendo solicitar corre√ß√£o ou exclus√£o conforme a LGPD."
        },
    ]
    
    print(f"‚úì Dataset com {len(CURRENT_EVALUATION_DATASET)} perguntas carregado")
    
    # ETAPA 5: Definir o System Prompt
    custom_system_prompt = """Voc√™ √© um assistente especializado em responder perguntas sobre o regulamento de cons√≥rcio.
Use APENAS as informa√ß√µes dispon√≠veis no contexto fornecido para responder.
Se a pergunta n√£o estiver relacionada ao contexto ou voc√™ n√£o encontrar a informa√ß√£o necess√°ria com base no contexto,
responda educadamente que n√£o pode ajudar com esse assunto espec√≠fico com base no material fornecido.
Mantenha suas respostas concisas, profissionais e baseadas nos fatos do contexto.
Responda em portugu√™s do Brasil.

Contexto Fornecido: {context}"""

    # ETAPA 6: Executar Avalia√ß√£o
    print(f"\nüéØ ETAPA 6: Executando avalia√ß√£o de {len(pipelines)} pipeline(s)...")
    if not pipelines:
        print("‚ùå Nenhum pipeline RAG foi configurado com sucesso. Encerrando avalia√ß√£o.")
        exit()

    evaluation_results_data = run_evaluation(
        evaluation_dataset=CURRENT_EVALUATION_DATASET,
        rag_pipelines_to_test=pipelines,
        llm=llm_instance,
        system_prompt=custom_system_prompt 
    )

    # ETAPA 7: Imprimir Relat√≥rio
    print_summary_report(evaluation_results_data)

    print("\n‚úÖ Avalia√ß√£o conclu√≠da com sucesso!")
    
    # Salvar resultados detalhados
    try:
        import json
        with open("evaluation_results.json", "w", encoding="utf-8") as f:
            json.dump(evaluation_results_data, f, ensure_ascii=False, indent=4)
        print("üíæ Resultados detalhados salvos em evaluation_results.json")
    except Exception as e:
        print(f"‚ùå Erro ao salvar resultados: {e}") 